{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training rapGPT: A Visually Friendly Guide\n",
    "\n",
    "This file is designed to provide a visually friendly process for training rapGPT. \n",
    "\n",
    "## Purpose of This File\n",
    "The purpose of this file is to offer detailed explanations of the training process, along with intermediate outputs to help understand how each step works. \n",
    "\n",
    "If you are looking for a script without the explanations and intermediate outputs, please refer to the corresponding script file: train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "#import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "#custom functions\n",
    "from scripts import utils, train_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters**\n",
    "\n",
    "These are the **final** hyperparamters used in the model.\n",
    "\n",
    "Hyperparameters have been adjusted accordingly to maximize model performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16  # how many independent sequences will be processed in parallel\n",
    "block_size = 512  # maximum context length (tokens)\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 200\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Eminem Lyrics Dataset from Kaggle\n",
    "\n",
    "## Overview\n",
    "\n",
    "The dataset contains information about Eminem's songs. The data consists of the following 5 columns:\n",
    "\n",
    "1. **Album Name**: The name of the album the song belongs to.\n",
    "2. **Song Name**: The name of the song.\n",
    "3. **Song Lyrics**: The lyrics of the song.\n",
    "4. **Album URL**: The URL of the album.\n",
    "5. **Song Views**: The number of views the song has received.\n",
    "6. **Release Date**: The date when the song was released.\n",
    "\n",
    "For our purpose, we will focus on the **Song Lyrics** column and ignore the other columns.\n",
    "\n",
    "## Dataset Link\n",
    "\n",
    "You can access the dataset [here](https://www.kaggle.com/datasets/aditya2803/eminem-lyrics/data).\n",
    "\n",
    "## Steps for Processing the Dataset\n",
    "\n",
    "We will be using **Pandas** for data manipulation and extraction of song lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Album_Name</th>\n",
       "      <th>Song_Name</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Album_URL</th>\n",
       "      <th>Views</th>\n",
       "      <th>Release_date</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Music To Be Murdered By: Side B</td>\n",
       "      <td>Alfred (Intro)</td>\n",
       "      <td>[Intro: Alfred Hitchcock]\\nThus far, this albu...</td>\n",
       "      <td>https://genius.com/albums/Eminem/Music-to-be-m...</td>\n",
       "      <td>24.3K</td>\n",
       "      <td>December 18, 2020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Music To Be Murdered By: Side B</td>\n",
       "      <td>Black Magic</td>\n",
       "      <td>[Chorus: Skylar Grey &amp; Eminem]\\nBlack magic, n...</td>\n",
       "      <td>https://genius.com/albums/Eminem/Music-to-be-m...</td>\n",
       "      <td>180.6K</td>\n",
       "      <td>December 18, 2020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Music To Be Murdered By: Side B</td>\n",
       "      <td>AlfredÃ¯Â¿Â½s Theme</td>\n",
       "      <td>[Verse 1]\\nBefore I check the mic (Check, chec...</td>\n",
       "      <td>https://genius.com/albums/Eminem/Music-to-be-m...</td>\n",
       "      <td>285.6K</td>\n",
       "      <td>December 18, 2020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Music To Be Murdered By: Side B</td>\n",
       "      <td>Tone Deaf</td>\n",
       "      <td>[Intro]\\nYeah, I'm sorry (Huh?)\\nWhat did you ...</td>\n",
       "      <td>https://genius.com/albums/Eminem/Music-to-be-m...</td>\n",
       "      <td>210.9K</td>\n",
       "      <td>December 18, 2020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Music To Be Murdered By: Side B</td>\n",
       "      <td>Book of Rhymes</td>\n",
       "      <td>[Intro]\\nI don't smile, I don't frown, get too...</td>\n",
       "      <td>https://genius.com/albums/Eminem/Music-to-be-m...</td>\n",
       "      <td>193.3K</td>\n",
       "      <td>December 18, 2020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Album_Name         Song_Name  \\\n",
       "0  Music To Be Murdered By: Side B    Alfred (Intro)   \n",
       "1  Music To Be Murdered By: Side B       Black Magic   \n",
       "2  Music To Be Murdered By: Side B  AlfredÃ¯Â¿Â½s Theme   \n",
       "3  Music To Be Murdered By: Side B         Tone Deaf   \n",
       "4  Music To Be Murdered By: Side B    Book of Rhymes   \n",
       "\n",
       "                                              Lyrics  \\\n",
       "0  [Intro: Alfred Hitchcock]\\nThus far, this albu...   \n",
       "1  [Chorus: Skylar Grey & Eminem]\\nBlack magic, n...   \n",
       "2  [Verse 1]\\nBefore I check the mic (Check, chec...   \n",
       "3  [Intro]\\nYeah, I'm sorry (Huh?)\\nWhat did you ...   \n",
       "4  [Intro]\\nI don't smile, I don't frown, get too...   \n",
       "\n",
       "                                           Album_URL   Views  \\\n",
       "0  https://genius.com/albums/Eminem/Music-to-be-m...   24.3K   \n",
       "1  https://genius.com/albums/Eminem/Music-to-be-m...  180.6K   \n",
       "2  https://genius.com/albums/Eminem/Music-to-be-m...  285.6K   \n",
       "3  https://genius.com/albums/Eminem/Music-to-be-m...  210.9K   \n",
       "4  https://genius.com/albums/Eminem/Music-to-be-m...  193.3K   \n",
       "\n",
       "        Release_date Unnamed: 6  \n",
       "0  December 18, 2020        NaN  \n",
       "1  December 18, 2020        NaN  \n",
       "2  December 18, 2020        NaN  \n",
       "3  December 18, 2020        NaN  \n",
       "4  December 18, 2020        NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"Raw Data/Eminem_Lyrics.csv\"\n",
    "data = pd.read_csv(PATH, sep='\\t', comment='#', encoding = \"ISO-8859-1\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Lyrics to a Text File\n",
    "Intermediary Files will be saved in case it may be used in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyrics have been written to Text File/eminem_lyrics.txt\n"
     ]
    }
   ],
   "source": [
    "output_file_path = 'Text File/'\n",
    "lyrics_file_name = 'eminem_lyrics.txt'\n",
    "lyrics = data['Lyrics']\n",
    "\n",
    "# Write lyrics to the text file, each lyric on a new line\n",
    "with open(output_file_path + lyrics_file_name, 'w', encoding='utf-8') as f:\n",
    "    for lyric in lyrics:\n",
    "        f.write(lyric + '\\n')\n",
    "\n",
    "print(f\"Lyrics have been written to {output_file_path + lyrics_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lyrics are separated into Intro, Outro, Chorus, Verse, etc. <br><br>\n",
    "**We are only interested in the [Verse] part of the lyrics since it contains the 'rap' portion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open lyrics text file \n",
    "with open(output_file_path + lyrics_file_name, 'r', encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "# Use regex to capture everything after '[Verse ...]' and before the next section\n",
    "verse_only = re.findall(r'\\[Verse.*?\\]\\n(.*?)(?=\\n\\[\\w|\\Z)', text, re.DOTALL)\n",
    "# Join the found text into a single string\n",
    "verse_only = '\\n'.join(verse_only)\n",
    "\n",
    "verse_file_name = 'verse_only.txt'\n",
    "# Output the result\n",
    "with open(output_file_path+verse_file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(verse_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Text\n",
    "1. Remove unwanted characters but keep newlines\n",
    "2. Normalize multiple spaces to a single space\n",
    "3. Remove trailing spaces before newlines\n",
    "4. Normalize multiple newlines to a single newline\n",
    "5. Convert to lower case\n",
    "\n",
    "**We are keeping newlines since it:**\n",
    "\n",
    "1. **Preserves Structure and Rhythm:**\n",
    "   - Rap lyrics are often structured in lines with rhymes, rhythms, and pauses. Keeping newlines helps the model learn this structure, making the generated lyrics feel more natural and rhythmic.\n",
    "2. **Improves Readability:**\n",
    "   - If the model generates lyrics with line breaks, it will be easier to read and evaluate during testing or usage.\n",
    "3. **Captures Line-Level Context:**\n",
    "   - By retaining newlines, the model can learn dependencies between consecutive lines without treating them as a continuous block of text.\n",
    "4. **Helps During Post-Processing:**\n",
    "   - You can always remove or modify newlines later if needed, but adding them back after training might be harder since the original structure would have been lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 180104\n"
     ]
    }
   ],
   "source": [
    "cleaned_verse_only = utils.preprocess_text_with_newlines(verse_only)\n",
    "cleaned_verse_only[:100]\n",
    "cleaned_verse_file_name = 'cleaned_verse_only.txt'\n",
    "# Output the result\n",
    "with open(output_file_path+cleaned_verse_file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(cleaned_verse_only)\n",
    "    \n",
    "words = cleaned_verse_only.split()\n",
    "# Get the number of words\n",
    "num_words = len(words)\n",
    "print(f\"Number of words: {num_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gpt2 BPE Tokenizer will be used to encode the text (Not used for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Load the GPT-2 tokenizer\\ngpt_tokenizer = tiktoken.get_encoding(\"gpt2\")\\n# Tokenize the text\\ntokens = gpt_tokenizer.encode(cleaned_verse_only)\\n\\n# Decode the tokens back to text\\n#decoded_text = tokenizer.decode(tokens[:10])\\n#print(\"Decoded text:\", decoded_text)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Load the GPT-2 tokenizer\n",
    "gpt_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "# Tokenize the text\n",
    "tokens = gpt_tokenizer.encode(cleaned_verse_only)\n",
    "\n",
    "# Decode the tokens back to text\n",
    "#decoded_text = tokenizer.decode(tokens[:10])\n",
    "#print(\"Decoded text:\", decoded_text)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer Training Plan\n",
    "\n",
    "- **Tokenizer Choice**: \n",
    "  - The trained tokenizer will be used with a vocab size of **30,000**, which is typically used for a model with a **small corpus**.\n",
    "- **Corpus Size**:\n",
    "  - The corpus that will be used for training has a size of **180,104 words**\n",
    "- **Tokenizer Types**:\n",
    "  - The corpus will be trained using both **BPE (Byte Pair Encoding)** since the model architecture wilk be based on the GPT model\n",
    "- **File Location**:\n",
    "  - The **train_tokenizer** script is saved in the `scripts` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create tokenizer\n",
    "bpe_tokenizer = train_tokens.train_tokenizer(input_files=[\"Text File/cleaned_verse_only.txt\"], vocab_size=30000, tokenizer_type=\"bpe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode **cleaned_verse_only** using the BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Tokens: [\"we're \", 'volatile ', \"i can't call it \", 'though\\n', \"it's like \", 'too ', 'large ', 'a ', 'pe', 'g and ']\n",
      "Len of Tokens: 116686\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the rap lyrics using the trained tokenizer\n",
    "bpe_tokenized_output = bpe_tokenizer.encode(cleaned_verse_only)\n",
    "# Print the tokenized output\n",
    "print(\"BPE Tokens:\", bpe_tokenized_output.tokens[:10])  # Prints the list of token strings\n",
    "print(\"Len of Tokens:\",  len(bpe_tokenized_output.ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try decoding the first 10 ids to verify if decoder is working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we're volatile i can't call it though it's like too large a pe g and\n"
     ]
    }
   ],
   "source": [
    "#get the numerical ids of the encoded toknes\n",
    "bpe_ids = bpe_tokenized_output.ids\n",
    "#get tokenized lyrics\n",
    "tokenized_lyrics = bpe_tokenized_output.tokens\n",
    "#try decoding first 10 ids\n",
    "output = bpe_tokenizer.decode(bpe_ids[:10])\n",
    "#remove empty spaces\n",
    "cleaned_output = re.sub(r'\\s+', ' ', output).strip()\n",
    "#print output\n",
    "print(cleaned_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting the data into test and validation sets\n",
    "90% of the data will be used for training, 10% for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([105017]), torch.Size([11669]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, val_data = utils.train_test_split(tokenizer_ids = bpe_ids, device= device)\n",
    "train_data.shape, val_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup for rapGPT\n",
    "\n",
    "We will be creating batches to train the data in parallel:\n",
    "\n",
    "- **Blocksize** = 512 (Each batch will contain 512 tokens at once)\n",
    "- **Batch size** = 16 (This indicates how many independent sequences will be processed in parallel)\n",
    "\n",
    "(16 batches are chosen based on max performance of my GPU: RTX4080 with 16GB VRAM)\n",
    "\n",
    "This setup allows efficient training by processing multiple sequences simultaneously, taking advantage of parallelization, while keeping the block size manageable for memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size of:  16\n",
      "block size of:  512\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = utils.get_batch(data = train_data, block_size = block_size, batch_size = batch_size, device= device)\n",
    "X_val, y_val = utils.get_batch(data = val_data, block_size = block_size, batch_size = batch_size, device= device)\n",
    "print(\"batch size of: \", X_train.shape[0])\n",
    "print(\"block size of: \", X_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure the labels of the data matches the train data at index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(X_train[0][1:], y_train[0][:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Creating the rapGPT model**\n",
    "\n",
    "## Overview\n",
    "This model uses a **decoder-only Transformer** architecture designed for autoregressive language modeling, where the model **generates text one token at a time** based on previously seen tokens. It consists of **stacked decoder blocks** with **multi-head self-attention** and **feedforward layers**.\n",
    "\n",
    "\n",
    "## Model Pipeline\n",
    "1. Input tokenized text â†’ Embedded + Positional Encoding\n",
    "2. Pass through **N stacked decoder blocks**\n",
    "3. Final linear projection â†’ Softmax â†’ Next token prediction\n",
    "\n",
    "## Key Features\n",
    "- **Causal Masking** prevents future token access.\n",
    "- **Scales with depth (N layers) and attention heads**.\n",
    "\n",
    "## Summary of Transformer Decoder Block\n",
    "```plaintext\n",
    "Input Embeddings â†’ LayerNorm â†’ Masked Multi-Head Self-Attention  â†’\n",
    "LayerNorm â†’ Feedforward Network â†’ Output\n",
    "```\n",
    "## Model Script\n",
    "refer to **gpt.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Components\n",
    "\n",
    "The following explains the architecture components of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Token and Positional Embeddings\n",
    "Each token is converted into a dense vector using an **embedding layer**, and a **positional encoding** is added to capture the order of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "# create token embeddings for each sample in the batch and block\n",
    "token_embedding = self.token_embeddings_table(input_tokens)  # (B, T, n_embd)\n",
    "        # create positional embeddings for each token in the block\n",
    "positional_embedding = self.position_embedding_table(torch.arange(T, device=device))  # (T, n_embd)\n",
    "x = token_embedding + positional_embedding  # (B, T, n_embd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Transformer Decoder Block\n",
    "Each decoder block consists of:\n",
    "\n",
    "1. **Masked Multi-Head Self-Attention**\n",
    "2. **Layer Normalization**\n",
    "3. **Feedforward Network (FFN)**\n",
    "4. **Residual Connections**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer Block: communication(multihead attention) followed by computation(FeedForward)\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head  # divide channel (feature embd) by num of heads\n",
    "        # self attention step\n",
    "        self.self_attention = MultiHeadAttention(n_head, head_size)\n",
    "        self.feedforward = FeedForward(n_embd)  # feedforward step\n",
    "        self.ln1 = nn.LayerNorm(normalized_shape=n_embd)  # first layer norm\n",
    "        self.ln2 = nn.LayerNorm(normalized_shape=n_embd)  # second layer norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Multi-Head Self-Attention\n",
    "Each token attends to **previous** tokens (causal attention), meaning it cannot see future tokens. The self-attention mechanism computes:\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} + M \\right) V $$\n",
    "\n",
    "where:\n",
    "- **Q** (queries), **K** (keys), **V** (values) are projections of the input.\n",
    "- **M** is a masking matrix that prevents attending to future tokens.\n",
    "- The softmax ensures attention weights sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "def forward(self, x):  # x: input for the model\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)  # (B, T, C)\n",
    "        q = self.query(x)  # (B, T, C)\n",
    "        # compute attention score\n",
    "        # (B, T, head_size) * (B, head_size, T) -> (B, T, T), # divide by sqrt(dim)\n",
    "        attn_score = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        # mask upper right triangle by converting 0 -> -inf for softmax\n",
    "        attn_score = attn_score.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        attn_score = F.softmax(attn_score, dim=-1)  # normalize using softmax\n",
    "\n",
    "        # apply weighted aggregation of values\n",
    "        v = self.value(x)  # (B, T, head_size)\n",
    "        out = attn_score @ v  # (B, T, head_size) * (B, T, T) -> (B, T, head_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Position-Wise Feedforward Network\n",
    "Each token is passed through a **2-layer MLP** with non-linearity:\n",
    "\n",
    "$$ FFN(x) = \\text{ReLU}(x W_1 + b_1) W_2 + b_2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Layer Normalization & Residual Connections\n",
    "Each sublayer (self-attention and FFN) has **LayerNorm + residual connections**:\n",
    "\n",
    "$$ X' = \\text{Self-Attention}(X + \\text{LayerNorm}(X)) $$\n",
    "$$ X'' = \\text{FFN}(X' + \\text{LayerNorm}(X')) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "def forward(self, x):\n",
    "        # pre-layer norm\n",
    "        # residual connections (add positional embeddings at the end)\n",
    "        # output = Activation(layer(X) + X)\n",
    "        \"\"\"\n",
    "        Input -> [LayerNorm] -> [Self-Attention] -> + (Residual Connection)\n",
    "        -> [LayerNorm] -> [Feedforward Network] -> + (Residual Connection) -> Output\n",
    "        \"\"\"\n",
    "        x = x + self.self_attention(self.ln1(x))\n",
    "        x = x + self.feedforward(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f) Output Projection (Logits & Softmax)\n",
    "The final hidden states are projected back to vocabulary size:\n",
    "\n",
    "$$ \\text{logits} = X W_o $$\n",
    "\n",
    "A **softmax function** then converts logits into probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "def generate(self, idx, max_new_tokens):\n",
    "        # input is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crops input to get the last 'block size' tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions,  loss will be ignored (uses forward function)\n",
    "            logits, loss = self(idx_cond, targets=None)\n",
    "            # focus only on the last time step, becomes (B, 1 ,C) last element in the time dimension -> last token\n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, 1, C)\n",
    "            # sample from distribution, (B, 1) single prediction for what comes next\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, next_token), dim=1)  # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Starting the Training Loop**\n",
    "\n",
    "## **Overview**\n",
    "The training loop is responsible for optimizing the model using **gradient-based learning**. This section explains the setup using the following hyperparameters:\n",
    "\n",
    "- **Batch Size**: `16` (number of sequences processed in parallel)\n",
    "- **Block Size**: `512` (maximum token context length)\n",
    "- **Optimizer**: `AdamW` (Adaptive optimization method)\n",
    "- **Learning Rate**: `3e-4`\n",
    "- **Vocab Size**: `30000`\n",
    "- **Number of Iterations**: `5000`\n",
    "- **Evaluation Interval**: Every `500` steps\n",
    "- **Dropout**: `0.2` (to prevent overfitting)\n",
    "- **Transformer Parameters**:\n",
    "  - Embedding Dimension: `512`\n",
    "  - Number of Attention Heads: `8`\n",
    "  - Number of Layers: `8`\n",
    "\n",
    "- **Optimizer**: Uses `AdamW` with a learning rate of `3e-4`.\n",
    "- **Training Loop (5000 iterations)**:\n",
    "   - Loads a batch from `train_data`.\n",
    "   - Computes the forward pass.\n",
    "   - Computes **loss** and backpropagates.\n",
    "   - Updates model parameters using `optimizer.step()`.\n",
    "   - Evaluates every `500` steps using `evaluate_loss()`.\n",
    "\n",
    "## Evaluation Process\n",
    "- Runs on `200` batches.\n",
    "- Computes the **average training and validation loss**.\n",
    "- Helps monitor model performance over time.\n",
    "\n",
    "## Training Script\n",
    "refer to **train.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of the Initial Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate the initial loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss should be around:  10.308952660644293\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "vocab_size = 30000\n",
    "initial_loss = -math.log(1/vocab_size)\n",
    "print(\"initial loss should be around: \", initial_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Detected: Training vs. Test Performance\n",
    "\n",
    "The model is performing **extremely well** on the training set but **worse** on the validation set.  \n",
    "This is a clear indication of **overfitting**, where the model memorizes training data but fails to generalize to unseen data.\n",
    "\n",
    "### Steps to Improve Generalization:\n",
    "- **Reduce Vocab Size** \n",
    "- **Increase Regularization** (e.g., L2 weight decay, dropout)\n",
    "- **Reduce Model Complexity** (e.g., fewer parameters or layers)\n",
    "\n",
    "By applying these techniques, we aim to achieve **better generalization** and improved test performance. ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique elements: tensor([    4,     6,     7,  ..., 29986, 29995, 29998], device='cuda:0')\n",
      "Number of unique elements: 3065\n"
     ]
    }
   ],
   "source": [
    "unique_elements = torch.unique(X_val)\n",
    "num_unique = unique_elements.numel()  # Get count of unique elements\n",
    "\n",
    "print(\"Unique elements:\", unique_elements)\n",
    "print(\"Number of unique elements:\", num_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 8192\n"
     ]
    }
   ],
   "source": [
    "unique_tokens = set(token for seq in y_train for token in seq)\n",
    "vocab_size = len(unique_tokens)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
